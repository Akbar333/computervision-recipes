{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy on SOTA datasets \n",
    "\n",
    "This notebook looks into reproducing the accuracy on datasets which are commonly used in SOTA approaches.\n",
    "\n",
    "\n",
    "### Dataset-specific setups:\n",
    "\n",
    "We follow the literature closely when setting up our experiments. See https://arxiv.org/abs/1511.06452:\n",
    "\n",
    "- CUB200-2011 dataset: \n",
    "   - all images in `data_rank` are used as both the query set and the gallery set. \n",
    "   - The images within the first 100 of the 200 classes from the dataset are used for training the DNN, the remaining 100 classes are used to compute Recall@K.\n",
    "\n",
    "- CARS196: \n",
    "   - The images within the first 98 of the 196 classes from the dataset are used for training the DNN, the remaining 98 classes are used to compute Recall@K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular python libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import scrapbook as sb\n",
    "\n",
    "# fast.ai\n",
    "import fastai\n",
    "from fastai.vision import (\n",
    "    accuracy,\n",
    "    cnn_learner,\n",
    "    DatasetType,\n",
    "    ImageList,\n",
    "    imagenet_stats,\n",
    "    models,\n",
    "    partial,\n",
    ")\n",
    "\n",
    "# Computer Vision repository\n",
    "sys.path.extend([\".\", \"../..\"])  # to access the utils_cv library\n",
    "from utils_cv.classification.data import Urls\n",
    "from utils_cv.classification.model import TrainMetricsRecorder\n",
    "from utils_cv.common.data import unzip_url\n",
    "from utils_cv.common.gpu import which_processor, db_num_workers\n",
    "from utils_cv.similarity.data import comparative_set_builder\n",
    "from utils_cv.similarity.metrics import (\n",
    "    compute_distances,\n",
    "    positive_image_ranks,\n",
    "    recall_at_k,\n",
    ")\n",
    "from utils_cv.similarity.model import compute_features, compute_features_learner\n",
    "from utils_cv.similarity.plot import (\n",
    "    plot_comparative_set,\n",
    "    plot_distances,\n",
    "    plot_ranks_distribution,\n",
    "    plot_recalls,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast.ai version = 1.0.57\n",
      "Torch is using GPU: Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fast.ai version = {fastai.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "We use popular datasets such as CUB-200-2011 or Cars196. These datasets come with a predefine split into training (used to refine the DNN) and into testing (used to compute Recall@K metric). Not that the train and test sets typically do not share any classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set dataset, model and evaluation parameters\n",
    "#DATA_FINETUNE_PATH = \"/home/pabuehle/Desktop/data/cub_200_2011/train\"\n",
    "#DATA_RANKING_PATH = \"/home/pabuehle/Desktop/data/cub_200_2011/test\"\n",
    "DATA_FINETUNE_PATH = \"/home/pabuehle/Desktop/data/cars196/train\"\n",
    "DATA_RANKING_PATH = \"/home/pabuehle/Desktop/data/cars196/test\"\n",
    "#DATA_FINETUNE_PATH = \"C:/Users/pabuehle/Desktop/data/cub_200_2011_subset3/train\"\n",
    "#DATA_RANKING_PATH = \"C:/Users/pabuehle/Desktop/data/cub_200_2011_subset3/test\"\n",
    "#DATA_FINETUNE_PATH = \"C:/Users/pabuehle/Desktop/data/cub_200_2011/train\"\n",
    "#DATA_RANKING_PATH = \"C:/Users/pabuehle/Desktop/data/cub_200_2011/test\"\n",
    "\n",
    "# DNN configuration and learning parameters\n",
    "EPOCHS_HEAD = 0\n",
    "EPOCHS_BODY = 0 #12\n",
    "LEARNING_RATE = 10* 1e-4\n",
    "BATCH_SIZE = 16\n",
    "ARCHITECTURE = models.resnet18\n",
    "IM_SIZE = 224 #300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our training data object, and split it to get a certain percentage (here 20%) assigned to a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\FINE-TUNING:\n",
      "Training set: 7249 images\n",
      "Validation set: 805 images\n"
     ]
    }
   ],
   "source": [
    "# Load images into fast.ai's ImageDataBunch object\n",
    "random.seed(642)\n",
    "data_finetune = (\n",
    "    ImageList.from_folder(DATA_FINETUNE_PATH)\n",
    "    .split_by_rand_pct(valid_pct=0.1, seed=20)\n",
    "    .label_from_folder()\n",
    "    .transform(size=IM_SIZE)\n",
    "    .databunch(bs=BATCH_SIZE, num_workers = db_num_workers())\n",
    "    .normalize(imagenet_stats)\n",
    ")\n",
    "\n",
    "print(f\"\"\"\\FINE-TUNING:\n",
    "Training set: {len(data_finetune.train_ds.x)} images\n",
    "Validation set: {len(data_finetune.valid_ds.x)} images\\\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model fine-tuning\n",
    "\n",
    "We begin by retrieving a [ResNet18](https://arxiv.org/pdf/1512.03385.pdf) CNN from fast.ai's library which is pre-trained on ImageNet, and fine-tune the model on our training set. We use the same training parameters and take the same approach as what we did in our [classification notebooks](https://github.com/microsoft/ComputerVision/tree/master/classification/notebooks), training first the (new) last layer only, and then the full DNN.\n",
    "\n",
    "Note how we train the DNN here on an image classification task but will use it as featurizer later for image similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(\n",
    "    data_finetune,\n",
    "    ARCHITECTURE,\n",
    "    metrics=[accuracy],\n",
    "    #callback_fns=[partial(TrainMetricsRecorder, show_graph=True)],\n",
    "    ps=0 #Leave dropout at zero. Higher values tend to perform significantly worse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 512 0.05\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastai.layers import FlattenedLoss\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "NUMBER_CLASSES = len(data_finetune.classes)\n",
    "FEATURE_DIM = learn.model[1][-1].in_features\n",
    "TEMPERATURE = 0.05\n",
    "#FC_WEIGHT = learn.model[1][6].weight\n",
    "\n",
    "print(NUMBER_CLASSES, FEATURE_DIM, TEMPERATURE)\n",
    "\n",
    "\n",
    "class NormSoftmaxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    L2 normalize weights and apply temperature scaling on logits.\n",
    "    https://github.com/azgo14/classification_metric_learning/blob/master/metric_learning/modules/losses.py#L7\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=FEATURE_DIM, num_instances=NUMBER_CLASSES, temperature=TEMPERATURE):\n",
    "        super(NormSoftmaxLoss, self).__init__()\n",
    "        #self.weight = FC_WEIGHT \n",
    "        self.weight = nn.Parameter(torch.Tensor(num_instances, dim)).cuda(torch.device(\"cuda\"))\n",
    "        # Initialization from nn.Linear (https://github.com/pytorch/pytorch/blob/v1.0.0/torch/nn/modules/linear.py#L129)\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.temperature = temperature\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    " \n",
    "    def forward(self, embeddings, instance_targets):\n",
    "        norm_emb = nn.functional.normalize(embeddings, dim=1)\n",
    "        norm_weight = nn.functional.normalize(self.weight, dim=1)\n",
    "        prediction_logits = nn.functional.linear(norm_emb, norm_weight)\n",
    "        loss = self.loss_fn(prediction_logits / self.temperature, instance_targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = FlattenedLoss(NormSoftmaxLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line achieves two things:\n",
    "# (1) Remove last fc layer so that input to loss function is the DNN feature, e.g. 512 dimensional.\n",
    "# (2) Instead, add normalization which is referred to in the paper in section X.X\n",
    "\n",
    "learn.model[1][6] = nn.LayerNorm(FEATURE_DIM, elementwise_affine=False)\n",
    "#learn.model[1][6] = nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/cv/lib/python3.6/site-packages/fastprogress/fastprogress.py:102: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "# Train the last layer using a larger rate since most of the DNN is fixed.\n",
    "learn.fit_one_cycle(EPOCHS_HEAD, 10* LEARNING_RATE) #, callbacks = [MyCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now unfreeze all the layers and fine-tuning the model more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(EPOCHS_BODY, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the ranking set and extract the DNN features for each image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\FINE-TUNING:\n",
      "Training set: 8131 images\n",
      "Validation set: 1 images\n"
     ]
    }
   ],
   "source": [
    "# Load images into fast.ai's ImageDataBunch object\n",
    "data_rank = (\n",
    "    ImageList.from_folder(DATA_RANKING_PATH)\n",
    "    .split_none()\n",
    "    .label_from_folder()\n",
    "    .transform(size=IM_SIZE)\n",
    "    .databunch(bs=BATCH_SIZE, num_workers = db_num_workers())\n",
    "    .normalize(imagenet_stats)\n",
    ")\n",
    "\n",
    "print(f\"\"\"\\FINE-TUNING:\n",
    "Training set: {len(data_rank.train_ds.x)} images\n",
    "Validation set: {len(data_rank.valid_ds.x)} images\\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Display example images\n",
    "#data_rank.show_batch(rows=3, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): AdaptiveConcatPool2d(\n",
       "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "    )\n",
       "    (1): Flatten()\n",
       "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line will allow us to extract the penultimate layer (ie 512 floating points vector) after running an image  through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n"
     ]
    }
   ],
   "source": [
    "# Use last layer as image representation\n",
    "embedding_layer = learn.model[1][-1] \n",
    "print(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='463' class='' max='509', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      90.96% [463/509 00:48<00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Compute DNN features for all validation images\n",
    "dnn_features = compute_features_learner(data_rank, DatasetType.Train, learn, embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Retrieval Example\n",
    "The cell below shows how to find and display the most similar images in the validation set for a given query image (which we also select from the validation set). This example is similar to the one shown in the [00_webcam.ipynb](https://github.com/microsoft/ComputerVision/tree/master/similarity/notebooks/00_webcam.ipynb) notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DNN feature for the query image\n",
    "query_im_path =  str(data_rank.train_ds.items[1])\n",
    "query_feature = dnn_features[query_im_path]\n",
    "print(f\"Query image path: {query_im_path}\")\n",
    "print(f\"Query feature dimension: {len(query_feature)}\")\n",
    "assert len(query_feature) == 512\n",
    "\n",
    "# Compute the distances between the query and all reference images\n",
    "distances = compute_distances(query_feature, dnn_features)\n",
    "plot_distances(distances, num_rows=1, num_cols=7, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative evaluation\n",
    "\n",
    "Our approach for quantitative evaluation is as follows:\n",
    "- We extract the features of each query image in the query set, and search the K most similar images in the gallery set. \n",
    "- If one of the K retrieved images have the same label with the query image, then that increases recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "rank1_count = 0\n",
    "labels = data_rank.train_ds.y\n",
    "#im_paths = [str(s) for s in list(data_rank.train_ds.items)]\n",
    "im_paths = data_rank.train_ds.items\n",
    "assert len(labels) == len(im_paths) == len(dnn_features)\n",
    "\n",
    "query_indices = range(len(im_paths))[::11] #only use every nth image to speed up accuracy computation\n",
    "\n",
    "# Loop over all images and use as query image once\n",
    "for query_index in query_indices:\n",
    "    if query_index % 1100 == 11:\n",
    "        acc = 100.0 * rank1_count / query_index\n",
    "        print(query_index, len(im_paths), acc)\n",
    "\n",
    "    # Get query information\n",
    "    query_im_path =  str(im_paths[query_index])\n",
    "    query_feature = dnn_features[query_im_path]\n",
    "    \n",
    "    # Compute distance to all images in the gallery set\n",
    "    # L2-normalizes all embeddings before computing the L2 distance.\n",
    "    # Note that the ranking is identical to setting `method = \"cosine\"` in the function below)\n",
    "    # (See: https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "    distances = compute_distances(query_feature, dnn_features) #, method = \"cosine\")\n",
    "\n",
    "    # Find the smallest distance\n",
    "    minDist = float('inf')\n",
    "    minDistIndex = None\n",
    "    for index, distance in enumerate(distances):\n",
    "        if index == query_index: #ignore the query image itself\n",
    "            continue\n",
    "\n",
    "        if distance[1] < minDist:\n",
    "            minDist = distance[1]\n",
    "            minDistIndex = index\n",
    "\n",
    "    # Compute rank\n",
    "    if labels[query_index] == labels[minDistIndex]:\n",
    "        rank1_count += 1\n",
    "        \n",
    "print(rank1_count, len(query_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall@1 accuracy = {:2.2f}\".format(100.0 * rank1_count / len(query_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log some outputs using scrapbook which are used during testing to verify correct notebook execution\n",
    "#sb.glue(\"median_rank\", median_rank)\n",
    "#sb.glue(\"random_rank\", random_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "### Example how to use callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.callback import Callback\n",
    "# from IPython.core.debugger import set_trace          \n",
    "              \n",
    "# class MyCallback():\n",
    "#     \"Base class for callbacks that want to record values, dynamically change learner params, etc.\"\n",
    "#     _order=0\n",
    "#     def on_train_begin(self, **kwargs)->None:\n",
    "#         \"To initialize constants in the callback.\"\n",
    "#         pass\n",
    "#     def on_epoch_begin(self, **kwargs)->None:  \n",
    "#         \"At the beginning of each epoch.\"\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "#     def on_batch_begin(self, **kwargs)->None:\n",
    "#         \"Set HP before the output and loss are computed.\"\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "#     def on_loss_begin(self, last_output, **kwargs)->None:\n",
    "#         \"Called after forward pass but before loss has been computed.\"\n",
    "#         #last_input == RGB image normalized\n",
    "#         #x = nn.functional.normalize(x, dim=1)\n",
    "#         #print(kwargs)\n",
    "#         #print(last_output)\n",
    "#         #set_trace()\n",
    "#         #last_output = 100*last_output\n",
    "#         pass\n",
    "              \n",
    "#     def on_backward_begin(self, **kwargs)->None:\n",
    "#         \"Called after the forward pass and the loss has been computed, but before backprop.\"\n",
    "#         pass\n",
    "#     def on_backward_end(self, **kwargs)->None:\n",
    "#         \"Called after backprop but before optimizer step. Useful for true weight decay in AdamW.\"\n",
    "#         pass\n",
    "#     def on_step_end(self, **kwargs)->None:\n",
    "#         \"Called after the step of the optimizer but before the gradients are zeroed.\"\n",
    "#         pass\n",
    "#     def on_batch_end(self, **kwargs)->None:\n",
    "#         \"Called at the end of the batch.\"\n",
    "#         pass\n",
    "#     def on_epoch_end(self, **kwargs)->None:\n",
    "#         \"Called at the end of an epoch.\"\n",
    "#         pass\n",
    "#     def on_train_end(self, **kwargs)->None:\n",
    "#         \"Useful for cleaning up things and saving files/models.\"\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example how to add L2-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyL2Norm(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MyL2Norm, self).__init__()\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         return torch.nn.functional.normalize(input, dim=1)\n",
    "\n",
    "# modules = []\n",
    "# modules.append(learn.model[1][0])\n",
    "# modules.append(learn.model[1][1])\n",
    "# modules.append(learn.model[1][2])\n",
    "# modules.append(learn.model[1][3])\n",
    "# modules.append(learn.model[1][4])\n",
    "# modules.append(learn.model[1][5])\n",
    "# modules.append(MyL2Norm())\n",
    "# modules.append(learn.model[1][6])\n",
    "# learn.model[1] = torch.nn.Sequential(*modules)\n",
    "# learn.model[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HACK TO DEACTIVATE BIAS IN LAST FC LAYER\n",
    "\n",
    "Hack: edit function bn_drop_lin() in /anaconda/envs/cv/lib/python3.6/site-packages/fastai/layers.py\n",
    "- Replace\n",
    "        layers.append(nn.Linear(n_in, n_out))\n",
    "- With: \n",
    "        if n_in != 512:\n",
    "            layers.append(nn.Linear(n_in, n_out))\n",
    "        else:\n",
    "            print(\"WARNING: SETTING BIAS TO FALSE IN bn_drop_lin\")\n",
    "            layers.append(nn.Linear(n_in, n_out, bias = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
